"""
Script de t√©l√©chargement et pr√©paration des datasets fran√ßais
Taille optimis√©e : ~3000-5000 exemples pour un entra√Ænement rapide
"""

import os
import pandas as pd
import numpy as np
# from datasets import load_dataset  # Temporairement d√©sactiv√© √† cause d'un conflit PyTorch
from sklearn.model_selection import train_test_split
import json

# Configuration
np.random.seed(42)
DATA_DIR = "data"
RAW_DIR = os.path.join(DATA_DIR, "raw")
PROCESSED_DIR = os.path.join(DATA_DIR, "processed")

# Cr√©er les dossiers si n√©cessaires
os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)

print("=" * 70)
print("T√âL√âCHARGEMENT DES DATASETS FRAN√áAIS")
print("=" * 70)


# =============================================================================
# 1. DATASET ALLOCINE - Pour le Sentiment (Critiques de films)
# =============================================================================
print("\nüì• 1. Cr√©ation d'un dataset de sentiment Allocine...")

# Cr√©er des exemples de critiques synth√©tiques (car load_dataset n√©cessite PyTorch)
# Vous pourrez enrichir avec de vraies donn√©es plus tard

allocine_data = []

positive_reviews = [
    "Film magnifique, j'ai ador√© ! Les acteurs sont excellents.",
    "Une merveille du cin√©ma, √† voir absolument ! üòç",
    "Quel chef-d'≈ìuvre ! Je suis sorti de la salle √©merveill√©.",
    "Excellente r√©alisation, sc√©nario captivant du d√©but √† la fin.",
    "Bravo ! Un film qui restera dans les m√©moires.",
] * 100

negative_reviews = [
    "Tr√®s d√©√ßu, le sc√©nario est pr√©visible et ennuyeux.",
    "Film m√©diocre, j'ai failli m'endormir. üò¥",
    "Quel g√¢chis ! Les acteurs jouent mal et l'histoire n'a aucun sens.",
    "Je ne recommande pas du tout, c'est une perte de temps.",
    "Nul, vraiment nul. Je regrette d'√™tre all√© le voir.",
] * 100

neutral_reviews = [
    "C'est correct, sans plus. Rien de m√©morable.",
    "Film moyen, certaines sc√®nes sont bonnes, d'autres moins.",
    "On a pass√© un moment correct, mais rien d'exceptionnel.",
    "Le film est regardable, mais je ne le reverrai pas.",
    "Pas mal dans l'ensemble, mais √ßa ne restera pas grav√©.",
] * 100

for text in positive_reviews[:500]:
    allocine_data.append({'text': text, 'sentiment': 2, 'sentiment_3class': 2})

for text in negative_reviews[:500]:
    allocine_data.append({'text': text, 'sentiment': 0, 'sentiment_3class': 0})

for text in neutral_reviews[:200]:
    allocine_data.append({'text': text, 'sentiment': 1, 'sentiment_3class': 1})

allocine_df = pd.DataFrame(allocine_data)

print(f"   ‚úì {len(allocine_df)} critiques cr√©√©es")
print(f"   Distribution : N√©gatif={sum(allocine_df['sentiment']==0)}, Neutre={sum(allocine_df['sentiment']==1)}, Positif={sum(allocine_df['sentiment']==2)}")

# Sauvegarder
allocine_df.to_csv(os.path.join(RAW_DIR, "allocine_sentiment.csv"), index=False)


# =============================================================================
# 2. DATASET EMOTIONS - Pour les √âmotions
# =============================================================================
print("\nüì• 2. Cr√©ation d'un dataset d'√©motions fran√ßais...")

# Comme il n'y a pas de gros dataset fran√ßais d'√©motions facilement accessible,
# on va cr√©er des exemples synth√©tiques bas√©s sur des patterns typiques
# (Vous pourrez les remplacer par de vraies donn√©es plus tard)

emotions_data = []

# Dictionnaire de phrases types par √©motion
emotion_examples = {
    'joie': [
        "Je suis trop content, c'est g√©nial ! üòä",
        "Quelle merveilleuse journ√©e, j'adore !",
        "Trop bien, je suis aux anges ! ‚ù§Ô∏è",
        "Super nouvelle, je suis ravi !",
        "C'est fantastique, je ne m'y attendais pas ! üéâ"
    ],
    'tristesse': [
        "Je suis vraiment triste aujourd'hui üò¢",
        "C'est d√©primant, rien ne va",
        "Je me sens si seul et abandonn√©",
        "Quelle d√©ception, je suis d√©vast√©",
        "Rien ne va plus, tout est noir üòî"
    ],
    'colere': [
        "J'en ai marre, c'est vraiment √©nervant ! üò°",
        "C'est inadmissible, je suis furieux !",
        "√áa suffit maintenant, je ne supporte plus !",
        "Quelle incomp√©tence, c'est r√©voltant !",
        "Je suis vraiment en col√®re contre toi ! üò†"
    ],
    'peur': [
        "J'ai vraiment peur, c'est angoissant üò®",
        "C'est effrayant, je suis terroris√©",
        "J'ai des frissons, c'est inqui√©tant",
        "Je suis anxieux, √ßa me stresse",
        "√áa fait peur, je suis paniqu√© üò∞"
    ],
    'surprise': [
        "Oh ! Je ne m'attendais pas √† √ßa ! üòÆ",
        "Quoi ?! C'est incroyable !",
        "Wow, quelle surprise !",
        "Je n'en crois pas mes yeux ! üò≤",
        "C'est inattendu, je suis choqu√© !"
    ],
    'degout': [
        "C'est d√©go√ªtant, beurk ! ü§¢",
        "J'ai la naus√©e, c'est r√©pugnant",
        "C'est √©c≈ìurant, je ne peux pas",
        "Quelle horreur, c'est immonde",
        "Beurk, c'est vraiment d√©gueulasse ü§Æ"
    ],
    'neutre': [
        "Le train arrive √† 15h.",
        "Il fait beau aujourd'hui.",
        "J'ai rendez-vous demain.",
        "La r√©union est √† 10h.",
        "Le magasin est ferm√© le dimanche."
    ]
}

# G√©n√©rer ~100 exemples par √©motion
for emotion_name, examples in emotion_examples.items():
    base_examples = examples * 20  # R√©p√©ter pour avoir ~100
    for i, text in enumerate(base_examples[:100]):
        emotions_data.append({
            'text': text,
            'emotion': emotion_name
        })

emotions_df = pd.DataFrame(emotions_data)

# Mapping des √©motions vers des indices
emotion_mapping = {
    'joie': 0, 'tristesse': 1, 'colere': 2, 'peur': 3,
    'surprise': 4, 'degout': 5, 'neutre': 6
}
emotions_df['emotion_id'] = emotions_df['emotion'].map(emotion_mapping)

print(f"   ‚úì {len(emotions_df)} exemples d'√©motions cr√©√©s")
print(f"   Distribution : {emotions_df['emotion'].value_counts().to_dict()}")

emotions_df.to_csv(os.path.join(RAW_DIR, "emotions.csv"), index=False)


# =============================================================================
# 3. DATASET IRONIE - Pour la D√©tection d'Ironie
# =============================================================================
print("\nüì• 3. Cr√©ation d'un dataset d'ironie...")

# Exemples d'ironie (vous pourrez enrichir avec de vraies donn√©es)
irony_data = []

ironic_examples = [
    "Super cette pluie, j'adore √™tre tremp√© ! üôÑ",
    "G√©nial, encore une r√©union inutile !",
    "Oh quelle joie, mon train est encore en retard !",
    "Fantastique, mon ordinateur a plant√© ! üòí",
    "J'adore attendre pendant des heures, vraiment !",
] * 50

non_ironic_examples = [
    "J'adore vraiment ce film, il est excellent !",
    "Quelle belle journ√©e, je suis content !",
    "Ce restaurant est vraiment bon, je recommande.",
    "J'ai pass√© un excellent week-end !",
    "Ce livre est passionnant, je ne peux pas m'arr√™ter.",
] * 50

for text in ironic_examples[:250]:
    irony_data.append({'text': text, 'is_ironic': 1})

for text in non_ironic_examples[:250]:
    irony_data.append({'text': text, 'is_ironic': 0})

irony_df = pd.DataFrame(irony_data)

print(f"   ‚úì {len(irony_df)} exemples d'ironie cr√©√©s")
print(f"   Distribution : Ironique={sum(irony_df['is_ironic']==1)}, Non-ironique={sum(irony_df['is_ironic']==0)}")

irony_df.to_csv(os.path.join(RAW_DIR, "irony.csv"), index=False)


# =============================================================================
# 4. FUSION DES DATASETS - Cr√©er le dataset multi-t√¢ches
# =============================================================================
print("\nüîÑ 4. Cr√©ation du dataset multi-t√¢ches combin√©...")

# Pour simplifier, on va cr√©er un dataset o√π chaque exemple a les 3 labels
# Prendre tous les exemples d'√©motions disponibles
combined_data = []

# Utiliser les √©motions comme base et ajouter sentiment + ironie
for idx, row in emotions_df.iterrows():
    # D√©duire le sentiment de l'√©motion
    if row['emotion'] in ['joie', 'surprise']:
        sentiment = 2  # positif
    elif row['emotion'] in ['tristesse', 'colere', 'peur', 'degout']:
        sentiment = 0  # n√©gatif
    else:
        sentiment = 1  # neutre
    
    # Ironie al√©atoire (20% de chance d'√™tre ironique)
    is_ironic = 1 if np.random.rand() > 0.8 else 0
    
    combined_data.append({
        'text': row['text'],
        'emotion': row['emotion'],
        'emotion_id': row['emotion_id'],
        'sentiment': sentiment,
        'is_ironic': is_ironic
    })

combined_df = pd.DataFrame(combined_data)

print(f"   ‚úì {len(combined_df)} exemples dans le dataset combin√©")

# Sauvegarder
combined_df.to_csv(os.path.join(RAW_DIR, "combined_multitask.csv"), index=False)


# =============================================================================
# 5. SPLIT TRAIN / VAL / TEST (Stratifi√©)
# =============================================================================
print("\n‚úÇÔ∏è 5. Split des donn√©es (70% train / 15% val / 15% test)...")

# Split stratifi√© sur l'√©motion (la t√¢che avec le plus de classes)
train_val, test = train_test_split(
    combined_df, 
    test_size=0.15, 
    random_state=42,
    stratify=combined_df['emotion_id']
)

train, val = train_test_split(
    train_val,
    test_size=0.176,  # 0.176 * 0.85 ‚âà 0.15 du total
    random_state=42,
    stratify=train_val['emotion_id']
)

print(f"   ‚úì Train : {len(train)} exemples")
print(f"   ‚úì Val   : {len(val)} exemples")
print(f"   ‚úì Test  : {len(test)} exemples")

# Sauvegarder les splits
train.to_csv(os.path.join(PROCESSED_DIR, "train.csv"), index=False)
val.to_csv(os.path.join(PROCESSED_DIR, "val.csv"), index=False)
test.to_csv(os.path.join(PROCESSED_DIR, "test.csv"), index=False)


# =============================================================================
# 6. STATISTIQUES GLOBALES
# =============================================================================
print("\nüìä 6. Statistiques globales...")

stats = {
    'total_examples': len(combined_df),
    'train_size': len(train),
    'val_size': len(val),
    'test_size': len(test),
    'emotion_distribution': combined_df['emotion'].value_counts().to_dict(),
    'sentiment_distribution': combined_df['sentiment'].value_counts().to_dict(),
    'irony_distribution': combined_df['is_ironic'].value_counts().to_dict(),
    'avg_text_length': int(combined_df['text'].str.len().mean()),
    'max_text_length': int(combined_df['text'].str.len().max()),
    'min_text_length': int(combined_df['text'].str.len().min())
}

# Sauvegarder les stats
with open(os.path.join(DATA_DIR, "dataset_stats.json"), 'w', encoding='utf-8') as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)

print(f"   ‚úì Statistiques sauvegard√©es dans {DATA_DIR}/dataset_stats.json")


# =============================================================================
# R√âSUM√â FINAL
# =============================================================================
print("\n" + "=" * 70)
print("‚úÖ T√âL√âCHARGEMENT TERMIN√â !")
print("=" * 70)
print(f"\nüìÅ Fichiers cr√©√©s :")
print(f"   ‚Ä¢ data/raw/allocine_sentiment.csv")
print(f"   ‚Ä¢ data/raw/emotions.csv")
print(f"   ‚Ä¢ data/raw/irony.csv")
print(f"   ‚Ä¢ data/raw/combined_multitask.csv")
print(f"   ‚Ä¢ data/processed/train.csv ({len(train)} exemples)")
print(f"   ‚Ä¢ data/processed/val.csv ({len(val)} exemples)")
print(f"   ‚Ä¢ data/processed/test.csv ({len(test)} exemples)")
print(f"   ‚Ä¢ data/dataset_stats.json")

print(f"\nüìä R√©sum√© :")
print(f"   ‚Ä¢ Total : {len(combined_df)} exemples")
print(f"   ‚Ä¢ Longueur moyenne des textes : {stats['avg_text_length']} caract√®res")
print(f"   ‚Ä¢ 7 √©motions, 3 sentiments, 2 classes d'ironie")

print(f"\nüéØ Prochaine √©tape :")
print(f"   Ouvrez le notebook 'notebooks/01_exploration_donnees.ipynb'")
print(f"   pour explorer visuellement ces donn√©es !")
print("=" * 70)
